import os
from dotenv import load_dotenv
from openai import AzureOpenAI # Azure openaië¡œ í™œìš©í•  ê²½ìš° OpenAI ëŒ€ì‹  AzureOpenAIë¥¼ importí•©ë‹ˆë‹¤.
import streamlit as st
import streamlit.components.v1 as components  # ì¶”ê°€
import pandas as pd
from io import StringIO
from azure.storage.blob import BlobServiceClient

from message_utils import init_messages
from blob_utils import load_data_from_blob

load_dotenv()  # Load environment variables from .env file

# Get environment variables
openai_endpoint = os.getenv("OPENAI_ENDPOINT")
openai_api_key = os.getenv("OPENAI_API_KEY")
chat_model = os.getenv("CHAT_MODEL")
embedding_model = os.getenv("EMBEDDING_MODEL")
search_endpoint = os.getenv("SEARCH_ENDPOINT")
search_api_key = os.getenv("SEARCH_API_KEY")
index_name = os.getenv("INDEX_NAME")

# initialize Azure OpenAI client
chat_client = AzureOpenAI(
    api_version="2024-12-01-preview",
    azure_endpoint=openai_endpoint,
    api_key=openai_api_key,
)

st.set_page_config(layout="wide")

init_messages()  # Initialize messages in session state

# if "messages" not in st.session_state:
#     # initialize prompt with system message
#     st.session_state.messages = [
#         {
#             "role": "system",
#             "content": """You are an agent that summarizes and explains subscriber performance data, 
#                         or provides detailed explanations about pricing plans."""
#         },
#     ]

def get_openai_response(messages):
    # Additional parameters to apply RAG pattern using the AI Search index
    if use_rag:
        rag_params = {
            "data_sources": [
                {
                    # he following params are used to search the index
                    "type": "azure_search",
                    "parameters": {
                        "endpoint": search_endpoint,
                        "index_name": index_name,
                        "authentication": {
                            "type": "api_key",
                            "key": search_api_key,
                        },
                        # The following params are used to vectorize the query
                        "query_type": "vector",
                        "embedding_dependency": {
                            "type": "deployment_name",
                            "deployment_name": embedding_model,
                        },
                    }
                }
            ],
        }

        #submit the chat reqest with RAG parameters
        response = chat_client.chat.completions.create(
            model=chat_model,
            messages=messages,
            extra_body=rag_params,  # Include RAG parameters
        )
    
    # If RAG is not used, just send the messages    
    else:
        response = chat_client.chat.completions.create(
            model=chat_model,
            messages=messages,
        )

    conpletion = response.choices[0].message.content
    return conpletion
    
# Handle user input
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")    
    
# space devide for user input
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ì›” ê°€ì…ì í˜„í™©")
    
    df = load_data_from_blob()
    st.markdown("CSV Viewer")
    st.dataframe(df)

    # # blob storage ë°ì´í„° ì¶”ì¶œ
    # storage_connect_string = os.getenv("STORAGE_CONNECT_STRING")
    # container_name = os.getenv("CONTAINER_NAME")
    # blob_name = os.getenv("BLOB_NAME")

    # blob_service = BlobServiceClient.from_connection_string(storage_connect_string)
    # blob_client = blob_service.get_blob_client(container=container_name, blob=blob_name)
    # csv_data = blob_client.download_blob().readall().decode("utf-8")

    # # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì¶œë ¥
    # df = pd.read_csv(StringIO(csv_data))
    # print(df.head())  # ë°ì´í„°í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
    # st.subheader("Blob Storage CSV Viewer")
    # st.dataframe(df)
        
with col2:
    st.header("ê°€ì…ì ì‹¤ì  ì˜ˆì¸¡ ìš”ì•½ Agent")
    st.markdown("ê°€ì…ì ì‹¤ì  ìµì›” ì˜ˆì¸¡ ë° ì´ë¥¼ í†µí•œ Insightsë¥¼ ì œê³µí•©ë‹ˆë‹¤.  \n"
            "ë˜í•œ, ëª¨ë°”ì¼ ìš”ê¸ˆì œì— ëŒ€í•´ ìì„¸í•œ ë‹µë³€ë„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.")
    
    if user_input != None:
        # Display message history
        for message in st.session_state.messages[1:]:
            st.chat_message(message["role"]).write(message["content"])
        
        # Save and display user message
        st.session_state.messages.append({"role": "user", "content": user_input})
        # user_message
        st.chat_message("user").write(user_input)
        
        # íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ RAG ì‚¬ìš©
        rag_keywords = ["ìš”ê¸ˆì œ", "ìš”ê¸ˆ", "í”Œëœ", "plan", "ê°€ê²©", "price"]
        use_rag = any(keyword in user_input for keyword in rag_keywords)
        
        # Generate and display assistant response
        with st.spinner("ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."):
            assistant_response = get_openai_response(st.session_state.messages)
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        # assistant_message
        st.chat_message("assistant").write(assistant_response)
        


# ê·¸ë¼íŒŒë‚˜ ëŒ€ì‹œë³´ë“œ
grafana_endpoint = os.getenv("GRAFANA_ENDPOINT")

st.sidebar.title("ğŸ“Š ì¶”ê°€ ì •ë³´")

st.sidebar.header("ğŸ“ˆ ëŒ€ì‹œë³´ë“œ ë³´ê¸°")
st.sidebar.markdown(f"ğŸ‘‰ [Grafana ëŒ€ì‹œë³´ë“œ ë°”ë¡œê°€ê¸°]({grafana_endpoint})", unsafe_allow_html=True)

